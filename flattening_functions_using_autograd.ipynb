{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening mathematical functions using `autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical functions come in all shapes and sizes and - moreover - we can often express indnividual equations in a variety of different ways.  This short section discusses a standardization technique called *function flattening*, which allows us to express any mathematical function  in the generic form $g\\left(\\mathbf{w}\\right)$ we have been using thus far.  Flattening is a particularly useful *pre-processing* step as it allows us to more broadly understand the fundamental optimization precepts we have / will see as well as more easily *implement* (in code) local optimization steps of the generic form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} + \\alpha \\, \\mathbf{d}^{k}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the following quadratic function $f$ of two $N\\times 1$ variables $\\mathbf{a}$ and $\\mathbf{b}$\n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\mathbf{a},\\mathbf{b} \\right) = \\left(\\sum_{n=1}^{N} a_nb_n\\right)^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is not written in the generic form $g\\left(\\mathbf{w}\\right)$ we have used throughout this Chapter (and which we will use throughout future Chapters as well), but of course all of the principles and algorithms we have seen / will see still apply to it.  So we can e.g., apply gradient descent to minimize the function.  To do this however we need to compute the gradient of $h$ with respect to  each input variable $\\mathbf{a}$ and $\\mathbf{b}$, and descend in each input variable as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}\n",
    "\\\n",
    "\\mathbf{a}^k = \\mathbf{a}^{k-1} - \\alpha \\, \\nabla_{\\mathbf{a}}\\,f\\left(\\mathbf{a}^{k-1},\\mathbf{b}^{k-1}\\right) \\\\\n",
    "\\mathbf{b}^k = \\mathbf{b}^{k-1} - \\alpha \\, \\nabla_{\\mathbf{b}}\\,f\\left(\\mathbf{a}^{k-1},\\mathbf{b}^{k-1}\\right) \\\\\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "in order to complete the $k^{th}$ step. \n",
    "\n",
    "There is a absolutely nothing wrong with this - it is a valid gradient descent step for the function given above.  It is however slightly more cumbersome to write - and implement - than a function of a single set of inputs like our standard $g\\left(\\mathbf{w}\\right)$ whose descent step can be written and implemented in a single line (one taken in $\\mathbf{w}$).  This annoyance is greatly amplified when dealing with functions of many inputs variables - which can be scalars, vectors, or even matrices - which we will regularly encounter during our machine learning voyage.  For such functions, in order to take a single gradient descent step we must *loop* over their many different input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully every mathematical function can be re-expressed so that *all* of its input variables are represented as a single contiguous array $\\mathbf{w}$, which alleivates this irritation.  For example in the example above we can easily see that by *re-indexing* entries of $\\mathbf{a}$ and $\\mathbf{b}$ using a single array as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "\\vdots \\\\\n",
    "a_N \\\\\n",
    "b_1 \\\\\n",
    "\\vdots \\\\\n",
    "b_N \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_N \\\\\n",
    "w_{N+1} \\\\\n",
    "\\vdots \\\\\n",
    "w_{2N} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "the function in equation (1) above can then be equivalently written as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = \\sum_{n=1}^N\\left(w_nw_{n+N}\\right)^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again note that all we have really done here is *re-indexed* the entries of both input vectors in a contiguous manner.  When expressed in this standardized manner we can both more easily reference optimization principles (which were scribed in Sections for functions in this standard form) and implement local optimization schemes like gradient descent in a less cumbersome way in a single line of algebra or `autograd` `Python` code, instead of requiring a loop over each input variable.  This variable re-indexing scheme is called *function flattening*, and can be applied to any mathematical function in principle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While performing the re-indexing required to flatten a function properly by hand for each and every function we come across is important, it (like derivative computation itself) is a repetitive and time consuming operations for human to perform themselves.  Therefore *in practice* we will automate this task, employing a flattening module from the `Python` `autograd` library.  This module can be imported from the `autograd` library (introduced in the previous Section) via the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import function flattening module from autograd\n",
    "from autograd.misc.flatten import flatten_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flatten a mathematical function scribed in `numpy` and `Python` called `f` - written in such a way that it takes in a single list containing all of its input variables - we then simply call the line below.  Here on the right hand side `weights` is a list of initializations for input variables to the function `f`.  The outputs `g`, `unflatten_func`, and `w` are the flattened version of `f`, a module to unflatten the input weights, and a flattened version of the intitial weights respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten an input function g\n",
    "g, unflatten_func, w = flatten_func(f, input_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flattened function and initialization can then be directly used with the gradient descent implementation given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 5. </span>  Gradient descent on a flattened function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we take the following function of several variables - a scalar, vector, and matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f\\left(a,\\mathbf{b},\\mathbf{C}\\right) = \n",
    "\\left(a + \\mathbf{z}^T\\mathbf{b} + \n",
    "\\mathbf{z}^T\\mathbf{C}\\mathbf{z} \n",
    "\\right)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and flatten it using the `autograd` module `flatten_func` in order to then minimize it using the gradient descent implementation given above.  Here the input variable $a$ is a scalar, $\\mathbf{b}$ is a $2 \\times 1$ vector, $\\mathbf{C}$ is a $2\\times 2$ matrix, and the non-variable vector $\\mathbf{z}$ is fixed at $\\mathbf{z} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"flattening.png\" width=700 height=250/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 1:</strong> <em> A figurative illustration of function flattening using the current example.\n",
    "</em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a `Python` version of the function defined above.  Note here that the input to this implementation is a list of the functions input variables (or weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "# Python implementation of the function above using autograd\n",
    "N = 2\n",
    "z = np.ones((N,1))\n",
    "def f(input_weights):\n",
    "    a = input_weights[0]\n",
    "    b = input_weights[1]\n",
    "    C = input_weights[2]\n",
    "    return (((a + np.dot(z.T,b) + np.dot(np.dot(z.T,C),z)))**2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `flatten_func` module as shown above we can then minimize the flattened version of this function properly.  In particular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create random initialization of each weight component\n",
    "a = np.random.randn(1,1); b = np.random.randn(N,1); C = np.random.randn(N,N)\n",
    "\n",
    "# store all weight components in single list\n",
    "w0 = [a,b,C]\n",
    "\n",
    "# flatten the input function f, and input initialization w0\n",
    "f_flat, unflatten_func, w0_flat = flatten_func(f, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get the flattened version of the function `f_flat` and flattened initialization `w0_flat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare the initial list of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.20096626]]), array([[-1.47248474],\n",
      "       [-0.06342298]]), array([[-0.36804953,  1.90142318],\n",
      "       [-0.94729369, -0.31869338]])]\n"
     ]
    }
   ],
   "source": [
    "print(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to our flattened version, which is now one contiguous list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20096626 -1.47248474 -0.06342298 -0.36804953  1.90142318 -0.94729369\n",
      " -0.31869338]\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "print (w0_flat)\n",
    "print (w0_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now more easily minimize this function by using its flattened version, avoiding the necessity to explicitly loop over each of its original weight components."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
